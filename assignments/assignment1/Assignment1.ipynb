{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmiR1l5xVMyAhl191hh4Ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak1909552/Artificial-Neural-Networks/blob/main/assignments/assignment1/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "dY4TdbyODu6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "TiQCgShVBSgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='mnist_mlp'></a>\n",
        "## The MNIST_MLP class\n",
        "\n",
        "This class has been modified to create models with hidden layers. The basic modification is the use of `nn.Sequential()` which allows layers and activation functions to be stacked together. As a result, `foward()` remains much the same and is easy to implement. This also allows the `model.parameters()` call in the main code to work without having to implement it in the `MNIST_MLP` class."
      ],
      "metadata": {
        "id": "fl5VkvUWCDxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, layer_sizes=[784, 10], activation=None):\n",
        "        super().__init__()\n",
        "        self.layer_sizes = layer_sizes \n",
        "        \n",
        "        # Different activations that you can use in forward() method.\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.identity = nn.Identity()\n",
        "\n",
        "        '''\n",
        "        self.activation is the activation function determined by the constructor argument\n",
        "        Defaults to self.relu\n",
        "        '''\n",
        "        \n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = self.sigmoid\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = self.tanh\n",
        "        elif activation == 'identity':\n",
        "            self.activation = self.identity\n",
        "        else:\n",
        "            self.activation = self.relu\n",
        "\n",
        "        '''\n",
        "        self.network represents the neural network to be constructed. \n",
        "\n",
        "        nn.Sequential accepts ordered dictionaries (dictionaries with the order of keys preserved)\n",
        "        to create a stacked network. \n",
        "\n",
        "        The structure of self.layer_dict is as follows:\n",
        "        {\n",
        "            'layer<i>' : nn.Linear(dim<i>, dim<i+1>),\n",
        "            'activation<i>' : self.activation,\n",
        "            ...\n",
        "        }\n",
        "\n",
        "        Based on the requirements of the questions, the last layer does not have an activation function.\n",
        "        '''\n",
        "        \n",
        "        self.layer_dict = OrderedDict()\n",
        "\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "          self.layer_dict[f'layer{i}'] = nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
        "          self.layer_dict[f'activation{i}'] = self.activation\n",
        "        \n",
        "        if len(layer_sizes) > 2:\n",
        "            del self.layer_dict[f'activation{len(layer_sizes) - 2}']\n",
        "            # self.layer_dict[f'activation{len(layer_sizes) - 2}'] = nn.Softmax(1)\n",
        "\n",
        "        self.network = nn.Sequential(self.layer_dict) \n",
        "\n",
        "    \n",
        "    def forward(self, input):\n",
        "\n",
        "        '''\n",
        "        The forward pass remains the same, and looks very clean :)\n",
        "        '''\n",
        "\n",
        "        input = input.view(-1, self.layer_sizes[0])\n",
        "        # Switch from activation maps to vectors\n",
        "        x = self.network(input)\n",
        "        return x\n",
        "    \n",
        "    # This function maps the network on the device that is passed as argument.\n",
        "    # If your device doesn't have a GPU, it set device='cpu'.\n",
        "    def set_device(self, device):\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "    \n",
        "    # This function trains the model on the data passed as arguments.\n",
        "\n",
        "    def fit(self, mnist_train_loader, num_epochs=1, mnist_valid_loader=None):\n",
        "        train_loss_history = []\n",
        "        train_acc_history = []\n",
        "        valid_loss_history = []\n",
        "        valid_acc_history = []\n",
        "        \n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            self.train() # Set to the training mode.\n",
        "            iter_loss = 0\n",
        "            iter_acc = 0\n",
        "            for i, (items, classes) in enumerate(mnist_train_loader):\n",
        "                items = Variable(items).to(self.device)\n",
        "                classes = Variable(classes).to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()     # Clear off the gradients from any past operation\n",
        "                outputs = self.forward(items)      # Do the forward pass\n",
        "                loss = self.criterion(outputs, classes) # Calculate the loss\n",
        "                loss.backward()           # Calculate the gradients with help of back propagation\n",
        "                self.optimizer.step()          # Ask the optimizer to adjust the parameters based on the gradients\n",
        "                iter_loss += loss.data # Accumulate the loss\n",
        "                iter_acc += (torch.max(outputs.data, 1)[1] == classes.data).sum()\n",
        "                print(\"\\r\", i + 1, \"/\", len(mnist_train_loader), \", Loss: \", loss.data/len(items), end=\"\")\n",
        "            train_loss_history += [iter_loss.cpu().detach().numpy()]\n",
        "            train_acc_history += [(iter_acc/len(mnist_train_loader.dataset)).cpu().detach().numpy()]\n",
        "            print(\"\\tTrain: \", train_loss_history[-1], train_acc_history[-1], end=\"\")\n",
        "            \n",
        "            self.eval() # Set to the evaluation mode.\n",
        "            iter_loss = 0\n",
        "            iter_acc = 0\n",
        "            for i, (items, classes) in enumerate(mnist_valid_loader):\n",
        "                items = Variable(items).to(self.device)\n",
        "                classes = Variable(classes).to(self.device)\n",
        "\n",
        "                outputs = self(items)      # Do the forward pass\n",
        "                iter_loss += self.criterion(outputs, classes).data\n",
        "                iter_acc += (torch.max(outputs.data, 1)[1] == classes.data).sum()\n",
        "            valid_loss_history += [iter_loss.cpu().detach().numpy()]\n",
        "            valid_acc_history += [(iter_acc/len(mnist_valid_loader.dataset)).cpu().detach().numpy()]\n",
        "            print(\"\\tValidation: \", valid_loss_history[-1], valid_acc_history[-1])\n",
        "        \n",
        "        return train_loss_history, train_acc_history, valid_loss_history, valid_acc_history\n",
        "\n"
      ],
      "metadata": {
        "id": "3xa4HOe6CKak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"perceptron\"></a>\n",
        "## The perceptron class\n",
        "\n",
        "This class implements the perceptron learning algorithm (PLA). It creates a single fully connected layer and initializes it with random weights. In the `forward`() method, the output is calculated as $w.x + b$ and the `torch.heaviside()` function is used as activation. In the `fit()` method, the weights and bias are calculated using the following formula:\n",
        "$$\n",
        "w \\leftarrow w + \\alpha \\hspace{1mm} \\times \\hspace{1mm} (y(v) \\hspace{1mm} - \\hspace{1mm} \\hat{y}(v)) \\hspace{1mm} \\times \\hspace{1mm} v\n",
        "$$\n",
        "\n",
        "The **bias** has been considered as a weight and inputs are affected accordingly.\n",
        "\n",
        "A simple description of algorithm:\n",
        "\n",
        "![PLA](https://miro.medium.com/max/1032/1*PbJBdf-WxR0Dd0xHvEoh4A.png)"
      ],
      "metadata": {
        "id": "gvSOy8fhCPBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class perceptron(nn.Module):\n",
        "    '''\n",
        "    Implements the perceptron learning algorithm (PLA) as defined by Rosenblat.\n",
        "    Inherits various properties from nn.Module.\n",
        "\n",
        "    methods:\n",
        "    \n",
        "    1) __init__():\n",
        "\n",
        "    Parameters:\n",
        "      layer_sizes: Integer array, length = 2\n",
        "      Defines the number of inputs and the number of perceptrons in a fully connected layer. Default = [784, 2]\n",
        "\n",
        "      learning_rate: float\n",
        "      Defines the learning rate of the algorithm. Default = 0.0001\n",
        "\n",
        "    Function:\n",
        "      simple constructor. Initializes the network with random weights and sets the learning_rate\n",
        "\n",
        "    2) forward():\n",
        "\n",
        "    Parameters:\n",
        "      inputs: Torch.Tensor()\n",
        "      Inputs of the mnist data_loader\n",
        "\n",
        "    Function:\n",
        "      Simple forward pass of perceptron. Computes the pre-activation as w.x + b. The activation\n",
        "      function is simply torch.Heaviside(). \n",
        "\n",
        "    Returns:\n",
        "      Torch.Tensor() containing the activation values of each tensor.\n",
        "\n",
        "    3) fit():\n",
        "    \n",
        "    Parameters:\n",
        "      train_loader: torch.utils.data.Dataloader()\n",
        "      Contains the training data \n",
        "\n",
        "      val_loader: torch.utils.data.DataLoader()\n",
        "      Contains the evaluation data. Defaults to None\n",
        "\n",
        "      epochs: int\n",
        "      Number of epochs to train the percpetron for. Defaults to 1.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, layer_sizes=[784, 2], learning_rate=0.0001):\n",
        "        super().__init__()\n",
        "        self.layer_sizes = layer_sizes     \n",
        "        self.fc1 = nn.Linear(layer_sizes[0], layer_sizes[1])\n",
        "        self.fc1.weight.data = torch.randn(layer_sizes[1], layer_sizes[0])\n",
        "        self.learning_rate = learning_rate\n",
        "    \n",
        "    def set_device(self, device):\n",
        "        self.device = device\n",
        "        # self.fc1.to(self.device)\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs_flattened = inputs.view(-1, self.layer_sizes[0])\n",
        "        pre_activation = self.fc1(inputs_flattened)\n",
        "        outputs = torch.heaviside(pre_activation, torch.tensor([0.0]).to(self.device))\n",
        "        return outputs\n",
        "    \n",
        "    def fit(self, train_loader, val_loader = None, epochs = 1):\n",
        "        \n",
        "        train_accuracy = []\n",
        "        test_accuracy = []\n",
        "\n",
        "        trens = len(train_loader.dataset)\n",
        "        vals = len(val_loader.dataset)\n",
        "        \n",
        "        self.train()\n",
        "        \n",
        "        iw = torch.randn(1,785).to(self.device)   ## store the inputs and 1\n",
        "        wb = torch.randn(2, 785).to(self.device)  ## store the weights and bias\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            tren = 0\n",
        "            for i, (items, classes) in enumerate(train_loader):\n",
        "                items = Variable(items).to(self.device)\n",
        "                classes = Variable(classes).to(self.device)\n",
        "                outputs = self(items)\n",
        "\n",
        "                ## optimization\n",
        "\n",
        "                weights = self.fc1.weight.data\n",
        "                bias = self.fc1.bias.data\n",
        "\n",
        "                for j in range(len(items)):\n",
        "                    actual = classes[j]\n",
        "                    predicted = outputs[j][0]\n",
        "                    \n",
        "\n",
        "                    if predicted != actual:\n",
        "                        tren = tren + 1\n",
        "                        inputs = items[j].view(-1, self.layer_sizes[0])\n",
        "\n",
        "                        iw[0][:-1] = inputs\n",
        "                        iw[0][-1] = torch.tensor([1.0])\n",
        "\n",
        "                        wb[0][:-1] = weights[0]\n",
        "                        wb[0][-1] = bias[0]\n",
        "                        wb[1][:-1] = weights[1]\n",
        "                        wb[1][-1] = bias[1]\n",
        "\n",
        "                        wb[0] = wb[0] + self.learning_rate*(actual - predicted)*iw\n",
        "                        wb[1] = wb[1] + self.learning_rate*(predicted - actual)*iw\n",
        "                        self.fc1.weight.data[0] = wb[0][:-1]\n",
        "                        self.fc1.weight.data[1] = wb[1][:-1]\n",
        "                        self.fc1.bias.data[0] = wb[0][-1]\n",
        "                        self.fc1.bias.data[1] = wb[1][-1] \n",
        "\n",
        "            tren_correct = trens - tren\n",
        "            train_accuracy += [(tren_correct / trens)]\n",
        "            print(f'Epoch {epoch + 1}. Trained with accuracy of {train_accuracy[epoch]:.4f}.')\n",
        "\n",
        "            self.eval()\n",
        "            val = 0\n",
        "            for i, (items, classes) in enumerate(val_loader):\n",
        "                items = Variable(items).to(self.device)\n",
        "                classes = Variable(classes).to(self.device)\n",
        "                outputs = self(items)\n",
        "\n",
        "                for j in range(len(outputs)):\n",
        "                    actual = classes[j]\n",
        "                    predicted = outputs[j][0]\n",
        "\n",
        "                    if predicted != actual:\n",
        "                        val = val + 1\n",
        "\n",
        "            val_correct = vals - val\n",
        "            test_accuracy += [(val_correct / vals)]\n",
        "            print(f'Epoch {epoch + 1}. Tested with accuracy of {test_accuracy[epoch]:.4f}.', end = '\\n\\n')\n",
        "\n",
        "        return train_accuracy, test_accuracy"
      ],
      "metadata": {
        "id": "nO_nxpgzCRqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `mnist_loader()` function"
      ],
      "metadata": {
        "id": "yjFk3u69CpRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_loader(batch_size=512, classes=None):\n",
        "    transform=transforms.Compose([transforms.ToTensor()])\n",
        "    mnist_train = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "    mnist_valid = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "    \n",
        "    # Select the classes which you want to train the classifier on.\n",
        "    if classes is not None:\n",
        "        mnist_train_idx = (mnist_train.targets == -1)\n",
        "        mnist_valid_idx = (mnist_valid.targets == -1)\n",
        "        for class_num in classes:\n",
        "            mnist_train_idx |= (mnist_train.targets == class_num)\n",
        "            mnist_valid_idx |= (mnist_valid.targets == class_num) \n",
        "        \n",
        "        mnist_train.targets = mnist_train.targets[mnist_train_idx]\n",
        "        mnist_valid.targets = mnist_valid.targets[mnist_valid_idx]\n",
        "        mnist_train.data = mnist_train.data[mnist_train_idx]\n",
        "        mnist_valid.data = mnist_valid.data[mnist_valid_idx]\n",
        "    \n",
        "    mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "    mnist_valid_loader = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "    return mnist_train_loader, mnist_valid_loader"
      ],
      "metadata": {
        "id": "stIu7bOtCsdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the device"
      ],
      "metadata": {
        "id": "FgdRFAB4CzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5-bML5aC0t9",
        "outputId": "64155619-380e-4bb1-c291-7132e9864ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1:"
      ],
      "metadata": {
        "id": "XuRM-TeVCc7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The solution implements the perceptron class. The training is done on the classes 0 and 1. The accuracies for training and evaluation are plotted. For details, refer to [the perceptron class](#perceptron) above. \n",
        "A sample run gives the following result:\n",
        "\n",
        "<img src=\"https://github.com/ak1909552/hostgifs/blob/main/Screen%20Shot%202022-10-06%20at%206.52.24%20PM.png?raw=true\" alt=\"drawing\" width=\"500\"/>\n"
      ],
      "metadata": {
        "id": "cm5d3_7OngtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512 # Reduce this if you get out-of-memory error\n",
        "    \n",
        "mnist_train_loader, mnist_valid_loader = mnist_loader(batch_size=batch_size, classes=[0,1])\n",
        "perc = perceptron()\n",
        "perc.set_device(device)\n",
        "tah, vah = perc.fit(mnist_train_loader, mnist_valid_loader, 20)\n",
        "plt.figure()\n",
        "plt.plot(tah, label='Train Accuracy')\n",
        "plt.plot(vah, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "plt.savefig('Question_1.pdf')"
      ],
      "metadata": {
        "id": "KiwDIGaQCihC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "\n",
        "The modified `MNIST_MLP` class is able to create a multi-layered model. The linear activation function available in torch is `nn.Identity()`. For details, refer to the [`MNIST_MLP`](#mnist_mlp) class. The model is trained for 20 epochs with the following sample result.\n",
        "\n",
        "<img src=\"https://github.com/ak1909552/hostgifs/blob/main/Screen%20Shot%202022-10-06%20at%206.53.51%20PM.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "5D6wECONMQZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "mnist_train_loader, mnist_valid_loader = mnist_loader(batch_size=batch_size)\n",
        "model = MNIST_MLP(layer_sizes=[784, 20, 20, 10], activation='identity')\n",
        "model.set_device(device)\n",
        "# Our loss function and Optimizer\n",
        "\n",
        "model.criterion = nn.CrossEntropyLoss()\n",
        "model.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #lr is the learning_rate\n",
        "# Train model for 20 epochs\n",
        "tlh, tah, vlh, vah = model.fit(mnist_train_loader, num_epochs=20, mnist_valid_loader=mnist_valid_loader)\n",
        "plt.figure()\n",
        "plt.plot(tah, label='Train Accuracy')\n",
        "plt.plot(vah, label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.tight_layout()\n",
        "plt.savefig('Question_2.pdf')"
      ],
      "metadata": {
        "id": "u8pr-4BLMVQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\n",
        "\n",
        "The code compares the performace of different activation functions, namely *relu*, *sigmoid* and *tanh*. The model performs best with *relu*, followed closely by *tanh*. Further, the graph also give us an idea of the learning rate. Here too, *relu* is seen to have a significantly better learning rate than the other activation functions. The following table summarizes the results:\n",
        "\n",
        "| Activation | Accuracy | Learning rate |\n",
        "|:-----------|:--------:|:-------------:|\n",
        "| relu | 90-92% | High |\n",
        "| tanh | 85-88% | Moderate |\n",
        "| sigmoid | 60-62% | Low |\n",
        "\n",
        "\n",
        "\n",
        "Following is a sample output:\n",
        "\n",
        "<img src=\"https://github.com/ak1909552/hostgifs/blob/main/Screen%20Shot%202022-10-06%20at%206.53.29%20PM.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "S1n1B70VRCUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "for activation in (['relu', 'sigmoid', 'tanh']):\n",
        "    # The model with activation\n",
        "    print(activation, end = '\\n\\n')\n",
        "    model = MNIST_MLP(layer_sizes=[784, 20, 20, 10], activation=activation)\n",
        "    model.set_device(device)\n",
        "    # Our loss function and Optimizer\n",
        "    model.criterion = nn.CrossEntropyLoss()\n",
        "    model.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #lr is the learning_rate\n",
        "    # Train model for 20 epochs\n",
        "    tlh, tah, vlh, vah = model.fit(mnist_train_loader, num_epochs=20, mnist_valid_loader=mnist_valid_loader)\n",
        "    # plt.plot(tah)\n",
        "    plt.plot(vah, label=activation + ' activation')\n",
        "plt.legend()\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.tight_layout()\n",
        "plt.savefig('Question_3.pdf')"
      ],
      "metadata": {
        "id": "j_aGH0WJRExB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "\n",
        "The code compares the performance of 3 models of increasing depths, 3, 4 and 5. All the models perform well. However, depth of 5 gives the best results, followed by 4 and 3. The learning rates of the models are also similar, indicating that there aren't too many sub-features to be learnt. Provided is a sample output:\n",
        "\n",
        "<img src=\"https://github.com/ak1909552/hostgifs/blob/main/Screen%20Shot%202022-10-06%20at%206.54.13%20PM.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "GzfdmTscRHjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes_list = [\n",
        "    [784, 20, 10],\n",
        "    [784, 20, 20, 10],\n",
        "    [784, 50, 30, 20, 10]\n",
        "]\n",
        "plt.figure()\n",
        "for d, layer_sizes in enumerate(layer_sizes_list):\n",
        "    print(layer_sizes, end = '\\n\\n')\n",
        "    # The model with activation\n",
        "    model = MNIST_MLP(layer_sizes=layer_sizes, activation='relu')\n",
        "    model.set_device(device)\n",
        "    # Our loss function and Optimizer\n",
        "    model.criterion = nn.CrossEntropyLoss()\n",
        "    model.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #lr is the learning_rate\n",
        "    # Train model for 20 epochs\n",
        "    tlh, tah, vlh, vah = model.fit(mnist_train_loader, num_epochs=20, mnist_valid_loader=mnist_valid_loader)\n",
        "    # plt.plot(tah)\n",
        "    plt.plot(vah, label='Depth = ' + str(d+3))\n",
        "plt.legend()\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.tight_layout()\n",
        "plt.savefig('Question_4.pdf')"
      ],
      "metadata": {
        "id": "CmvNguc7RJmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5\n",
        "\n",
        "The code compares the performance of varying width of the hidden layers, 5, 20 and 50. The performance is best with width of 50, followed by 20 and 5. Width of 50 also gives a significant increase in the learning rate. The following table summarizes the results:\n",
        "\n",
        "| Width | Accuracy | Learning rate |\n",
        "|:-----------|:--------:|:-------------:|\n",
        "| 5 | 55-60% | Low |\n",
        "| 20 | 88-90% | Moderate |\n",
        "| 50 | 89-93% | High |\n",
        "\n",
        "Provided is a sample output:\n",
        "\n",
        "<img src=\"https://github.com/ak1909552/hostgifs/blob/main/Screen%20Shot%202022-10-06%20at%206.54.42%20PM.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "OoiZ_oNnRebR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes_list = [\n",
        "        [784, 5, 5, 10],\n",
        "        [784, 20, 20, 10],\n",
        "        [784, 50, 50, 10]\n",
        "    ]\n",
        "plt.figure()\n",
        "for layer_sizes in layer_sizes_list:\n",
        "    # The model with activation\n",
        "    print(layer_sizes, end = '\\n\\n')\n",
        "    model = MNIST_MLP(layer_sizes=layer_sizes, activation='relu')\n",
        "    model.set_device(device)\n",
        "    # Our loss function and Optimizer\n",
        "    model.criterion = nn.CrossEntropyLoss()\n",
        "    model.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #lr is the learning_rate\n",
        "    # Train model for 20 epochs\n",
        "    tlh, tah, vlh, vah = model.fit(mnist_train_loader, num_epochs=20, mnist_valid_loader=mnist_valid_loader)\n",
        "    # plt.plot(tah)\n",
        "    plt.plot(vah, label='Width = ' + str(layer_sizes[1]))\n",
        "plt.legend()\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.tight_layout()\n",
        "plt.savefig('Question_5.pdf')"
      ],
      "metadata": {
        "id": "jVK-61B8Rgf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to visit the [colab notebook](https://colab.research.google.com/drive/1TzFBMGDndZNgzCzyrfK_qmKAeid61ypU?usp=sharing)."
      ],
      "metadata": {
        "id": "UE7WkosNDNX_"
      }
    }
  ]
}